import os
import re
from collections import Counter
from stt_module import transcribe_audio
from emotion_module import classify_emotion
from chat_module import chat_with_gpt
from tts_module import speak_text
import requests  # ìƒë‹¨ importì— ì¶”ê°€
import json


def save_message_to_api(text, emotion, mode="VOICE", user_no=1, chat_no=1):
    payload = {
        "content": text,
        "mode": mode,
        "summary": emotion,
        "userNo": user_no,
        "chatNo": chat_no
    }
    headers = {"Content-Type": "application/json"}

    print("ğŸ“¤ ì „ì†¡ payload:", json.dumps(payload, ensure_ascii=False))

    
    response = requests.post(
        "http://localhost:3000/messages/send",
        json=payload,
        headers=headers
    )
    
    if response.status_code in [200, 201]:
        print("âœ… ë©”ì‹œì§€ ì €ì¥ ì„±ê³µ!")
    else:
        print(f"âŒ ë©”ì‹œì§€ ì €ì¥ ì‹¤íŒ¨: {response.status_code}, {response.text}")



# âœ… ê°ì • ë¦¬í¬íŠ¸ í´ë˜ìŠ¤
class EmotionReport:
    def __init__(self):
        self.emotion_log = []
        self.text_log = []

    def add_turn(self, text):
        self.text_log.append(text)
        emotion = classify_emotion(text)
        self.emotion_log.append(emotion)
        return emotion

    def get_emotion_summary(self):
        total = len(self.emotion_log)
        counts = Counter(self.emotion_log)
        return {
            emotion: round((count / total) * 100, 1)
            for emotion, count in counts.items()
        }

    def get_top_keywords(self, top_n=5):
        all_text = ' '.join(self.text_log).lower()
        words = re.findall(r'\b[ê°€-í£a-zA-Z]+\b', all_text)
        stopwords = set(['ê·¸ë¦¬ê³ ', 'ê·¸ë˜ì„œ', 'í•˜ì§€ë§Œ', 'ê·¸ëƒ¥', 'ë‚˜ëŠ”', 'ë„ˆëŠ”', 'ì´ê±´', 'ì €ê±´', 'ë­ì§€', 'ì´ê²Œ', 'ì €ê²Œ', 'ê²ƒ'])
        filtered = [w for w in words if w not in stopwords]
        return [word for word, _ in Counter(filtered).most_common(top_n)]

    def generate_parenting_tip(self):
        emotion_summary = self.get_emotion_summary()
        top_keywords = self.get_top_keywords()

        prompt = f"""
        ë‹¹ì‹ ì€ ì•„ë™ ì‹¬ë¦¬ ì „ë¬¸ê°€ì´ì ë¶€ëª¨ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ë‹¤ìŒì€ ì•„ì´ì™€ì˜ ëŒ€í™”ì—ì„œ ë¶„ì„ëœ ê°ì • ìš”ì•½ê³¼ ì£¼ìš” í‚¤ì›Œë“œì…ë‹ˆë‹¤.

        ê°ì • ìš”ì•½: {emotion_summary}
        ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(top_keywords)}

        ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ì´ì˜ ê°ì • ìƒíƒœë¥¼ ì´í•´í•˜ê³ ,
        ë¶€ëª¨ê°€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•˜ë©´ ì¢‹ì„ì§€ í•œêµ­ì–´ë¡œ ë”°ëœ»í•˜ê³  ì‹¤ìš©ì ì¸ ìœ¡ì•„ íŒì„ 3~5ì¤„ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.
        """
        return chat_with_gpt(prompt, emotion="neutral")
    
    
# âœ… ìŒì„± ë³´ê³ ì„œ ì‹¤í–‰ í•¨ìˆ˜
def run_emotion_report():
    report = EmotionReport()
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    audio_dir = os.path.join(BASE_DIR, "audio_inputs")

    if not os.path.exists(audio_dir):
        print(f"âŒ ë””ë ‰í† ë¦¬ {audio_dir} ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    audio_files = sorted([f for f in os.listdir(audio_dir) if f.endswith(".wav")],
                         key=lambda x: int(os.path.splitext(x)[0]))

    for filename in audio_files:
        audio_path = os.path.join(audio_dir, filename)
        print(f"\nğŸ¤ íŒŒì¼ [{filename}] ìŒì„± ì¸ì‹ ì¤‘...")
        user_text = transcribe_audio(audio_path)
        print("ğŸ‘¶ ì¸ì‹ëœ í…ìŠ¤íŠ¸:", user_text)
        emotion = report.add_turn(user_text)
        print(f"ğŸ§  ê°ì • ë¶„ì„ ê²°ê³¼: {emotion}")
        reply = chat_with_gpt(user_text, emotion)
        print(f"ğŸ¤– GPT ì‘ë‹µ: {reply}")
        speak_text(reply)
        save_message_to_api(user_text, emotion, user_no=1)
        save_message_to_api(reply, "neutral", user_no=2)

    print("\nğŸ“Š ì „ì²´ ê°ì • ìš”ì•½:")
    for emo, perc in report.get_emotion_summary().items():
        print(f"- {emo}: {perc}%")
    print("\nğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ:")
    for i, kw in enumerate(report.get_top_keywords(), 1):
        print(f"{i}. {kw}")
    print("\nğŸ‘¨â€ğŸ‘©â€ğŸ‘§ ìœ¡ì•„ ì†”ë£¨ì…˜ ì œì•ˆ:")
    print(report.generate_parenting_tip())

    report = EmotionReport()

    # ğŸ“Œ audio_inputs í´ë” ê²½ë¡œë¥¼ main.py ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œë¡œ ì„¤ì •
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    audio_dir = os.path.join(BASE_DIR, "audio_inputs")

    if not os.path.exists(audio_dir):
        print(f"âŒ ë””ë ‰í† ë¦¬ {audio_dir} ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    # ìˆ«ì ìˆœìœ¼ë¡œ .wav íŒŒì¼ ì •ë ¬
    audio_files = sorted(
        [f for f in os.listdir(audio_dir) if f.endswith(".wav")],
        key=lambda x: int(os.path.splitext(x)[0])
    )

    for filename in audio_files:
        audio_path = os.path.join(audio_dir, filename)
        print(f"\nğŸ¤ íŒŒì¼ [{filename}] ìŒì„± ì¸ì‹ ì¤‘...")

        # 1. ìŒì„± ì¸ì‹
        user_text = transcribe_audio(audio_path)
        print("ğŸ‘¶ ì¸ì‹ëœ í…ìŠ¤íŠ¸:", user_text)

        # 2. ê°ì • ë¶„ì„
        emotion = report.add_turn(user_text)
        print(f"ğŸ§  ê°ì • ë¶„ì„ ê²°ê³¼: {emotion}")

        # 3. GPT ì‘ë‹µ
        reply = chat_with_gpt(user_text, emotion)
        print(f"ğŸ¤– GPT ì‘ë‹µ: {reply}")

        # 4. TTS ì‘ë‹µ ì¶œë ¥
        speak_text(reply)

        # âœ… ë©”ì‹œì§€ ì €ì¥
        save_message_to_api(user_text, emotion, user_no=1)  # ì‚¬ìš©ìì˜ ë©”ì‹œì§€
        save_message_to_api(reply, "neutral", user_no=2)   # GPTì˜ ì‘ë‹µ (ì¤‘ë¦½ ê°ì •ìœ¼ë¡œ ì €ì¥)

    # âœ… ì „ì²´ í†µê³„ ë° ìœ¡ì•„ ì†”ë£¨ì…˜
    print("\nğŸ“Š ì „ì²´ ê°ì • ìš”ì•½:")
    for emo, perc in report.get_emotion_summary().items():
        print(f"- {emo}: {perc}%")

    print("\nğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ:")
    for i, kw in enumerate(report.get_top_keywords(), 1):
        print(f"{i}. {kw}")

    print("\nğŸ‘¨â€ğŸ‘©â€ğŸ‘§ ìœ¡ì•„ ì†”ë£¨ì…˜ ì œì•ˆ:")
    print(report.generate_parenting_tip())



# âœ… ì˜ìƒ ë³´ê³ ì„œ ì‹¤í–‰ í•¨ìˆ˜
def run_behavior_report(video_path="./recorded_video.mp4"):
    from behavior_report import BehaviorReport
    b_report = BehaviorReport(video_path)
    b_report.analyze()
    print("\nğŸ¥ í–‰ë™ ë¶„ì„ ë³´ê³ ì„œ:")
    print(b_report.generate_report_text())


# âœ… CLI ì§„ì…ì  ì¶”ê°€
def cli():
    import argparse, os
    ap = argparse.ArgumentParser()
    ap.add_argument("--mode", choices=["voice", "video"], required=True)
    ap.add_argument("--video", help="ë¶„ì„í•  mp4 ê²½ë¡œ (video ëª¨ë“œ í•„ìˆ˜)")
    args = ap.parse_args()

    if args.mode == "voice":
        run_emotion_report()
    else:
        if not args.video:
            raise SystemExit("--video ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤. (ì˜ˆ: --video ./uploads/xxx.mp4)")
        # ìƒëŒ€ê²½ë¡œ ë³´ì •
        if not os.path.isabs(args.video):
            base = os.path.dirname(os.path.abspath(__file__))
            args.video = os.path.normpath(os.path.join(base, args.video))
        run_behavior_report(args.video)

if __name__ == "__main__":
    # ìë™ ì‹¤í–‰ ì—†ìŒ (í”„ë¡ íŠ¸/ë°±ì—ì„œ í•„ìš”í•  ë•Œë§Œ cli()ë¡œ í˜¸ì¶œ)
    # ì˜ˆ) python ravo_emotion/main.py --mode video --video ./uploads/xxx.mp4
    pass



# # âœ… ì‹¤í–‰
# if __name__ == "__main__":
# #    main()
#     # ìŒì„± í˜ì´ì§€ â†’ run_emotion_report()
#     run_behavior_report("./ravo_emotion/test.mp4")
#     pass